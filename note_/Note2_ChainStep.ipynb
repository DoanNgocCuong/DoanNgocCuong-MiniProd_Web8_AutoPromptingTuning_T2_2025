{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dưới đây là một hướng dẫn chi tiết kèm code mẫu cho cách triển khai quy trình tự động tạo prompt, tạo test cases, đánh giá kết quả, nhận feedback từ người dùng và cập nhật prompt. Chúng ta sẽ triển khai:\n",
    "\n",
    "1. **Backend (FastAPI)**  \n",
    "   - Tích hợp “4o-mini” để tạo prompt và test cases.  \n",
    "   - Sử dụng “o1-min” (hoặc một model LLM mạnh hơn) để đánh giá prompt.  \n",
    "   - Thực hiện vòng lặp tự động (iterative loop) cho đến khi đạt yêu cầu (ví dụ: accuracy ≥ 90% hoặc đạt số vòng tối đa).  \n",
    "   - Cung cấp endpoint để nhận feedback từ người dùng nhằm cập nhật prompt.\n",
    "\n",
    "2. **Frontend (React)**  \n",
    "   - Giao diện cho người dùng nhập dữ liệu (format, samples, conditions).  \n",
    "   - Hiển thị prompt được tạo, bảng test cases và thông số đánh giá (accuracy, response time, iteration, history).  \n",
    "   - Cho phép người dùng nhập feedback và gửi về backend.  \n",
    "   - Cho phép người dùng “update prompt” dựa trên feedback và lặp lại quá trình.\n",
    "\n",
    "Dưới đây là ví dụ code cho từng phần.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Backend (FastAPI)\n",
    "\n",
    "### Cấu trúc thư mục mẫu:\n",
    "```\n",
    "backend/\n",
    "├── main.py\n",
    "├── utils.py         # (các hàm giả lập: generate_prompt, generate_test_cases, evaluate_prompt, update_prompt)\n",
    "├── models.py        # (định nghĩa Pydantic models)\n",
    "├── requirements.txt\n",
    "└── Dockerfile\n",
    "```\n",
    "\n",
    "### File: `backend/models.py`\n",
    "```python\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "\n",
    "class Sample(BaseModel):\n",
    "    input: str\n",
    "    output: str\n",
    "\n",
    "class TestCase(BaseModel):\n",
    "    input: str\n",
    "    expected_output: str\n",
    "    actual_output: str\n",
    "    is_correct: bool\n",
    "    similarity_score: float\n",
    "\n",
    "class OptimizationHistory(BaseModel):\n",
    "    iteration: int\n",
    "    accuracy: float\n",
    "    response_time: float\n",
    "\n",
    "class PromptRequest(BaseModel):\n",
    "    format: str\n",
    "    samples: List[Sample]\n",
    "    conditions: Optional[str] = None\n",
    "    iteration: int = 0  # vòng lặp hiện tại\n",
    "\n",
    "class PromptResponse(BaseModel):\n",
    "    generated_prompt: str\n",
    "    test_cases: List[TestCase]\n",
    "    accuracy: float\n",
    "    response_time: float\n",
    "    iteration: int\n",
    "    optimization_history: List[OptimizationHistory]\n",
    "\n",
    "class FeedbackRequest(BaseModel):\n",
    "    prompt: str\n",
    "    feedback: str\n",
    "```\n",
    "\n",
    "### File: `backend/utils.py`\n",
    "Trong file này, chúng ta mô phỏng các hàm sử dụng “4o-mini” để tạo prompt & test cases, và “o1-min” để đánh giá prompt.  \n",
    "Bạn có thể thay thế các hàm này bằng tích hợp model thực tế.\n",
    "```python\n",
    "import random\n",
    "import time\n",
    "from typing import List\n",
    "from models import Sample, TestCase\n",
    "\n",
    "# Giả lập hàm tạo prompt từ samples, format, conditions (sử dụng \"4o-mini\")\n",
    "def generate_prompt_from_samples(fmt: str, samples: List[Sample], conditions: str) -> str:\n",
    "    base = f\"Format: {fmt}\\n\"\n",
    "    sample_str = \"\\n\".join([f\"Input: {s.input} => Output: {s.output}\" for s in samples])\n",
    "    cond_str = f\"Conditions: {conditions}\" if conditions else \"\"\n",
    "    prompt = f\"{base}{sample_str}\\n{cond_str}\"\n",
    "    return prompt\n",
    "\n",
    "# Giả lập hàm tạo test cases từ prompt (sử dụng \"4o-mini\")\n",
    "def generate_test_cases(prompt: str) -> List[TestCase]:\n",
    "    # Ví dụ: tạo 5 test cases với dữ liệu ngẫu nhiên\n",
    "    test_cases = []\n",
    "    for i in range(1, 6):\n",
    "        expected = f\"Expected output {i}\"\n",
    "        # Giả lập actual_output bằng cách thêm chút nhiễu ngẫu nhiên\n",
    "        actual = expected if random.random() > 0.3 else f\"Wrong output {i}\"\n",
    "        is_correct = (actual == expected)\n",
    "        similarity = 1.0 if is_correct else random.uniform(0.5, 0.7)\n",
    "        test_cases.append(\n",
    "            TestCase(\n",
    "                input=f\"Test input {i}\",\n",
    "                expected_output=expected,\n",
    "                actual_output=actual,\n",
    "                is_correct=is_correct,\n",
    "                similarity_score=similarity\n",
    "            )\n",
    "        )\n",
    "    return test_cases\n",
    "\n",
    "# Giả lập hàm đánh giá prompt (sử dụng model LLM mạnh hơn \"o1-min\")\n",
    "def evaluate_prompt(prompt: str, test_cases: List[TestCase]) -> (float, float):\n",
    "    # Thời gian phản hồi giả lập\n",
    "    start = time.time()\n",
    "    # Accuracy tính theo tỉ lệ các test case đúng, có thể tính trung bình similarity\n",
    "    correct_cases = sum(1 for tc in test_cases if tc.is_correct)\n",
    "    accuracy = correct_cases / len(test_cases)\n",
    "    response_time = time.time() - start\n",
    "    return accuracy, response_time\n",
    "\n",
    "# Giả lập hàm cập nhật prompt dựa trên feedback và kết quả đánh giá\n",
    "def update_prompt(current_prompt: str, test_cases: List[TestCase], accuracy: float) -> str:\n",
    "    # Ở đây ta mô phỏng cập nhật bằng cách thêm thông tin \"Updated\" vào prompt\n",
    "    updated = current_prompt + \"\\n[Updated Prompt based on evaluation feedback]\"\n",
    "    return updated\n",
    "```\n",
    "\n",
    "### File: `backend/main.py`\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "from models import PromptRequest, PromptResponse, FeedbackRequest, OptimizationHistory\n",
    "from utils import generate_prompt_from_samples, generate_test_cases, evaluate_prompt, update_prompt\n",
    "\n",
    "app = FastAPI(title=\"Auto Prompting Tool API\")\n",
    "\n",
    "MAX_ITERATION = 5  # số vòng lặp tối đa\n",
    "\n",
    "@app.post(\"/api/generate-prompt\", response_model=PromptResponse)\n",
    "def generate_prompt_endpoint(request: PromptRequest):\n",
    "    # Bước 1: Tự viết Prompt (dùng 4o-mini)\n",
    "    prompt = generate_prompt_from_samples(request.format, request.samples, request.conditions)\n",
    "    iteration = request.iteration\n",
    "    optimization_history = []\n",
    "\n",
    "    # Bước 2: Tự tạo test cases (dùng 4o-mini)\n",
    "    test_cases = generate_test_cases(prompt)\n",
    "\n",
    "    # Bước 3: Tự đánh giá kết quả (dùng model LLM mạnh hơn)\n",
    "    accuracy, response_time = evaluate_prompt(prompt, test_cases)\n",
    "    optimization_history.append(OptimizationHistory(\n",
    "        iteration=iteration,\n",
    "        accuracy=accuracy,\n",
    "        response_time=response_time\n",
    "    ))\n",
    "\n",
    "    # Lặp lại bước 1 và 2 (tự cập nhật prompt) nếu chưa đạt điều kiện (accuracy < 90%)\n",
    "    while accuracy < 0.9 and iteration < MAX_ITERATION:\n",
    "        iteration += 1\n",
    "        # Cập nhật prompt dựa trên feedback nội bộ (điều chỉnh tự động)\n",
    "        prompt = update_prompt(prompt, test_cases, accuracy)\n",
    "        test_cases = generate_test_cases(prompt)\n",
    "        accuracy, response_time = evaluate_prompt(prompt, test_cases)\n",
    "        optimization_history.append(OptimizationHistory(\n",
    "            iteration=iteration,\n",
    "            accuracy=accuracy,\n",
    "            response_time=response_time\n",
    "        ))\n",
    "\n",
    "    return PromptResponse(\n",
    "        generated_prompt=prompt,\n",
    "        test_cases=test_cases,\n",
    "        accuracy=accuracy,\n",
    "        response_time=response_time,\n",
    "        iteration=iteration,\n",
    "        optimization_history=optimization_history\n",
    "    )\n",
    "\n",
    "@app.post(\"/api/feedback\")\n",
    "def feedback_endpoint(request: FeedbackRequest):\n",
    "    # Bước 4: Nhận feedback từ người dùng và lưu lại hoặc xử lý logic cập nhật\n",
    "    # Ở đây chỉ demo log feedback. Thực tế có thể lưu vào cơ sở dữ liệu và kích hoạt quá trình update prompt.\n",
    "    print(\"Feedback nhận được:\", request.feedback)\n",
    "    return {\"message\": \"Feedback đã được nhận!\"}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=8000, reload=True)\n",
    "```\n",
    "\n",
    "### File: `backend/requirements.txt`\n",
    "```\n",
    "fastapi\n",
    "uvicorn\n",
    "pydantic\n",
    "```\n",
    "\n",
    "### File: `backend/Dockerfile`\n",
    "```dockerfile\n",
    "FROM python:3.10-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "COPY . .\n",
    "\n",
    "EXPOSE 8000\n",
    "\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Frontend (React)\n",
    "\n",
    "### Cấu trúc thư mục mẫu:\n",
    "```\n",
    "frontend/\n",
    "├── public/\n",
    "│   └── index.html\n",
    "├── src/\n",
    "│   └── App.tsx\n",
    "├── package.json\n",
    "└── Dockerfile\n",
    "```\n",
    "\n",
    "### File: `frontend/public/index.html`\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"vi\">\n",
    "<head>\n",
    "  <meta charset=\"UTF-8\">\n",
    "  <title>Auto Prompting Tool</title>\n",
    "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n",
    "</head>\n",
    "<body>\n",
    "  <div id=\"root\"></div>\n",
    "</body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "### File: `frontend/src/App.tsx`\n",
    "Ví dụ dưới đây bao gồm các bước:  \n",
    "- Gửi dữ liệu để tạo prompt và nhận kết quả (bao gồm prompt, test cases, accuracy, response time, iteration, optimization history).  \n",
    "- Hiển thị kết quả và bảng test cases.  \n",
    "- Nhận feedback từ người dùng qua một form riêng.\n",
    "```tsx\n",
    "import React, { useState, ChangeEvent, FormEvent } from 'react'\n",
    "import axios from 'axios'\n",
    "\n",
    "interface Sample {\n",
    "  input: string\n",
    "  output: string\n",
    "}\n",
    "\n",
    "interface TestCase {\n",
    "  input: string\n",
    "  expected_output: string\n",
    "  actual_output: string\n",
    "  is_correct: boolean\n",
    "  similarity_score: number\n",
    "}\n",
    "\n",
    "interface OptimizationHistory {\n",
    "  iteration: number\n",
    "  accuracy: number\n",
    "  response_time: number\n",
    "}\n",
    "\n",
    "interface PromptResponseData {\n",
    "  generated_prompt: string\n",
    "  test_cases: TestCase[]\n",
    "  accuracy: number\n",
    "  response_time: number\n",
    "  iteration: number\n",
    "  optimization_history: OptimizationHistory[]\n",
    "}\n",
    "\n",
    "function App() {\n",
    "  const [format, setFormat] = useState('')\n",
    "  const [samples, setSamples] = useState<Sample[]>([{ input: '', output: '' }])\n",
    "  const [conditions, setConditions] = useState('')\n",
    "  const [generatedPrompt, setGeneratedPrompt] = useState('')\n",
    "  const [testCases, setTestCases] = useState<TestCase[]>([])\n",
    "  const [accuracy, setAccuracy] = useState(0)\n",
    "  const [responseTime, setResponseTime] = useState(0)\n",
    "  const [iteration, setIteration] = useState(0)\n",
    "  const [optimizationHistory, setOptimizationHistory] = useState<OptimizationHistory[]>([])\n",
    "  const [feedback, setFeedback] = useState('')\n",
    "  const [isLoading, setIsLoading] = useState(false)\n",
    "\n",
    "  const API_URL = import.meta.env.VITE_API_URL || 'http://localhost:8000'\n",
    "\n",
    "  const handleAddSample = () => {\n",
    "    setSamples([...samples, { input: '', output: '' }])\n",
    "  }\n",
    "\n",
    "  const handleSampleChange = (index: number, field: 'input' | 'output', value: string) => {\n",
    "    const newSamples = [...samples]\n",
    "    newSamples[index][field] = value\n",
    "    setSamples(newSamples)\n",
    "  }\n",
    "\n",
    "  const handleTextChange = (e: ChangeEvent<HTMLTextAreaElement | HTMLInputElement>) => {\n",
    "    const { id, value } = e.target\n",
    "    if (id === 'format') setFormat(value)\n",
    "    if (id === 'conditions') setConditions(value)\n",
    "    if (id === 'feedback') setFeedback(value)\n",
    "  }\n",
    "\n",
    "  const handleInputChange = (\n",
    "    e: ChangeEvent<HTMLInputElement>,\n",
    "    index: number,\n",
    "    field: 'input' | 'output'\n",
    "  ) => {\n",
    "    handleSampleChange(index, field, e.target.value)\n",
    "  }\n",
    "\n",
    "  // Gửi dữ liệu để tạo prompt và nhận kết quả (các bước 1-3 và tự lặp)\n",
    "  const handleGeneratePrompt = async (e: FormEvent) => {\n",
    "    e.preventDefault()\n",
    "    try {\n",
    "      setIsLoading(true)\n",
    "      const response = await axios.post<PromptResponseData>(`${API_URL}/api/generate-prompt`, {\n",
    "        format,\n",
    "        samples,\n",
    "        conditions,\n",
    "        iteration: 0\n",
    "      })\n",
    "      const data = response.data\n",
    "      setGeneratedPrompt(data.generated_prompt)\n",
    "      setTestCases(data.test_cases)\n",
    "      setAccuracy(data.accuracy)\n",
    "      setResponseTime(data.response_time)\n",
    "      setIteration(data.iteration)\n",
    "      setOptimizationHistory(data.optimization_history)\n",
    "    } catch (error) {\n",
    "      console.error('Error:', error)\n",
    "    } finally {\n",
    "      setIsLoading(false)\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // Gửi feedback từ người dùng (Bước 4)\n",
    "  const handleFeedbackSubmit = async () => {\n",
    "    try {\n",
    "      await axios.post(`${API_URL}/api/feedback`, {\n",
    "        prompt: generatedPrompt,\n",
    "        feedback: feedback\n",
    "      })\n",
    "      alert('Feedback đã được gửi!')\n",
    "    } catch (error) {\n",
    "      console.error('Feedback error:', error)\n",
    "    }\n",
    "  }\n",
    "\n",
    "  return (\n",
    "    <div className=\"container mx-auto p-4\">\n",
    "      <h1 className=\"text-2xl font-bold mb-4\">Auto Prompting Tool</h1>\n",
    "      \n",
    "      <form onSubmit={handleGeneratePrompt}>\n",
    "        <div className=\"mb-4\">\n",
    "          <label htmlFor=\"format\" className=\"block mb-2\">Prompt Format:</label>\n",
    "          <textarea\n",
    "            id=\"format\"\n",
    "            className=\"w-full p-2 border rounded\"\n",
    "            value={format}\n",
    "            onChange={handleTextChange}\n",
    "          />\n",
    "        </div>\n",
    "\n",
    "        <div className=\"mb-4\">\n",
    "          <label className=\"block mb-2\">Samples:</label>\n",
    "          {samples.map((sample, index) => (\n",
    "            <div key={index} className=\"mb-2 flex gap-2\">\n",
    "              <input\n",
    "                type=\"text\"\n",
    "                className=\"flex-1 p-2 border rounded\"\n",
    "                placeholder=\"Input\"\n",
    "                value={sample.input}\n",
    "                onChange={(e) => handleInputChange(e, index, 'input')}\n",
    "              />\n",
    "              <input\n",
    "                type=\"text\"\n",
    "                className=\"flex-1 p-2 border rounded\"\n",
    "                placeholder=\"Output\"\n",
    "                value={sample.output}\n",
    "                onChange={(e) => handleInputChange(e, index, 'output')}\n",
    "              />\n",
    "            </div>\n",
    "          ))}\n",
    "          <button type=\"button\" className=\"bg-blue-500 text-white px-4 py-2 rounded\" onClick={handleAddSample}>\n",
    "            Add Sample\n",
    "          </button>\n",
    "        </div>\n",
    "\n",
    "        <div className=\"mb-4\">\n",
    "          <label htmlFor=\"conditions\" className=\"block mb-2\">Additional Conditions:</label>\n",
    "          <textarea\n",
    "            id=\"conditions\"\n",
    "            className=\"w-full p-2 border rounded\"\n",
    "            value={conditions}\n",
    "            onChange={handleTextChange}\n",
    "          />\n",
    "        </div>\n",
    "\n",
    "        <button\n",
    "          type=\"submit\"\n",
    "          className={`bg-green-500 text-white px-4 py-2 rounded ${isLoading ? 'opacity-50 cursor-not-allowed' : ''}`}\n",
    "          disabled={isLoading}\n",
    "        >\n",
    "          {isLoading ? 'Generating...' : 'Generate Prompt'}\n",
    "        </button>\n",
    "      </form>\n",
    "\n",
    "      {generatedPrompt && (\n",
    "        <div className=\"mt-4\">\n",
    "          <h2 className=\"text-xl font-bold\">Generated Prompt:</h2>\n",
    "          <pre className=\"p-4 bg-gray-100 rounded whitespace-pre-wrap\">{generatedPrompt}</pre>\n",
    "          \n",
    "          <div className=\"mt-2 grid grid-cols-3 gap-4\">\n",
    "            <div>\n",
    "              <p className=\"font-bold\">Accuracy:</p>\n",
    "              <p>{(accuracy * 100).toFixed(2)}%</p>\n",
    "            </div>\n",
    "            <div>\n",
    "              <p className=\"font-bold\">Response Time:</p>\n",
    "              <p>{responseTime.toFixed(2)}s</p>\n",
    "            </div>\n",
    "            <div>\n",
    "              <p className=\"font-bold\">Iterations:</p>\n",
    "              <p>{iteration}</p>\n",
    "            </div>\n",
    "          </div>\n",
    "        </div>\n",
    "      )}\n",
    "\n",
    "      {optimizationHistory.length > 0 && (\n",
    "        <div className=\"mt-4\">\n",
    "          <h2 className=\"text-xl font-bold\">Optimization History:</h2>\n",
    "          <div className=\"grid gap-4\">\n",
    "            {optimizationHistory.map((history, index) => (\n",
    "              <div key={index} className=\"p-4 bg-gray-100 rounded\">\n",
    "                <p><strong>Iteration {history.iteration}:</strong></p>\n",
    "                <p>Accuracy: {(history.accuracy * 100).toFixed(2)}%</p>\n",
    "                <p>Response Time: {history.response_time.toFixed(2)}s</p>\n",
    "              </div>\n",
    "            ))}\n",
    "          </div>\n",
    "        </div>\n",
    "      )}\n",
    "\n",
    "      {testCases.length > 0 && (\n",
    "        <div className=\"mt-4\">\n",
    "          <h2 className=\"text-xl font-bold\">Test Cases:</h2>\n",
    "          <div className=\"grid gap-4\">\n",
    "            {testCases.map((test, index) => (\n",
    "              <div key={index} className={`p-4 rounded ${test.is_correct ? 'bg-green-100' : 'bg-red-100'}`}>\n",
    "                <p><strong>Input:</strong> {test.input}</p>\n",
    "                <p><strong>Expected:</strong> {test.expected_output}</p>\n",
    "                <p><strong>Actual:</strong> {test.actual_output}</p>\n",
    "                <p><strong>Similarity:</strong> {(test.similarity_score * 100).toFixed(2)}%</p>\n",
    "              </div>\n",
    "            ))}\n",
    "          </div>\n",
    "        </div>\n",
    "      )}\n",
    "\n",
    "      {/* Phần nhận feedback từ người dùng (Bước 4) */}\n",
    "      <div className=\"mt-6\">\n",
    "        <h2 className=\"text-xl font-bold mb-2\">User Feedback</h2>\n",
    "        <textarea\n",
    "          id=\"feedback\"\n",
    "          className=\"w-full p-2 border rounded mb-2\"\n",
    "          placeholder=\"Nhập feedback của bạn\"\n",
    "          value={feedback}\n",
    "          onChange={handleTextChange}\n",
    "        />\n",
    "        <button className=\"bg-purple-500 text-white px-4 py-2 rounded\" onClick={handleFeedbackSubmit}>\n",
    "          Submit Feedback\n",
    "        </button>\n",
    "      </div>\n",
    "    </div>\n",
    "  )\n",
    "}\n",
    "\n",
    "export default App\n",
    "```\n",
    "\n",
    "### File: `frontend/package.json`\n",
    "Đảm bảo có các dependencies cần thiết (ví dụ React, TypeScript, axios, …)\n",
    "```json\n",
    "{\n",
    "  \"name\": \"auto-prompting-tool\",\n",
    "  \"version\": \"1.0.0\",\n",
    "  \"private\": true,\n",
    "  \"scripts\": {\n",
    "    \"start\": \"vite\",\n",
    "    \"build\": \"vite build\"\n",
    "  },\n",
    "  \"dependencies\": {\n",
    "    \"axios\": \"^1.3.0\",\n",
    "    \"react\": \"^18.2.0\",\n",
    "    \"react-dom\": \"^18.2.0\"\n",
    "  },\n",
    "  \"devDependencies\": {\n",
    "    \"typescript\": \"^4.9.0\",\n",
    "    \"vite\": \"^4.0.0\",\n",
    "    \"@types/react\": \"^18.0.0\",\n",
    "    \"@types/react-dom\": \"^18.0.0\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### File: `frontend/Dockerfile`\n",
    "```dockerfile\n",
    "FROM node:18-alpine\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY package*.json ./\n",
    "RUN npm install\n",
    "\n",
    "COPY . .\n",
    "\n",
    "EXPOSE 5173\n",
    "\n",
    "CMD [\"npm\", \"run\", \"start\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Tổng Kết Quy Trình\n",
    "\n",
    "1. **Tự viết Prompt (sử dụng model 4o-mini):**  \n",
    "   - Người dùng nhập thông tin (format, samples, conditions) ở giao diện frontend.  \n",
    "   - Gửi request đến endpoint `/api/generate-prompt` ở backend.\n",
    "2. **Tự tạo Test Cases (dùng 4o-mini):**  \n",
    "   - Backend sinh test cases dựa trên prompt vừa tạo.\n",
    "3. **Tự đánh giá kết quả (dùng model LLM mạnh hơn):**  \n",
    "   - Backend đánh giá prompt dựa trên các test case, tính toán accuracy và response time.  \n",
    "   - Nếu chưa đạt yêu cầu (accuracy < 90%), backend tự cập nhật prompt (bước 1 và 2) theo một vòng lặp cho đến đạt điều kiện hoặc đến số vòng tối đa.\n",
    "4. **User đánh giá và feedback lại:**  \n",
    "   - Frontend hiển thị kết quả (prompt, test cases, evaluation metrics, optimization history).  \n",
    "   - Người dùng nhập feedback và gửi qua endpoint `/api/feedback`.\n",
    "5. **Update Prompt:**  \n",
    "   - Backend có thể sử dụng feedback của người dùng (hoặc logic tự động) để cập nhật prompt.\n",
    "6. **Lặp lại bước 3, 4, 5:**  \n",
    "   - Quy trình đánh giá và cập nhật sẽ được lặp lại dựa trên yêu cầu hoặc trigger từ phía người dùng.\n",
    "\n",
    "Với hướng dẫn và code mẫu trên, bạn có thể triển khai hệ thống tự động tạo prompt, đánh giá và tối ưu theo quy trình được mô tả. Bạn có thể mở rộng logic, tích hợp các model thực sự và lưu trữ dữ liệu tùy theo nhu cầu dự án."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
